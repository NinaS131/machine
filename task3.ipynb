{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, stem\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Chef</td>\n",
       "      <td>I'm sorry boys.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
       "1     10       1      Kyle                        Going away? For how long?\\n\n",
       "2     10       1      Stan                                         Forever.\\n\n",
       "3     10       1      Chef                                  I'm sorry boys.\\n\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'All-seasons.csv'\n",
    "df = pandas.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим героев с наибольшим числом реплик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mr. Mackey</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharon</th>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenny</th>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chef</th>\n",
       "      <td>917</td>\n",
       "      <td>917</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. Garrison</th>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randy</th>\n",
       "      <td>2467</td>\n",
       "      <td>2467</td>\n",
       "      <td>2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Butters</th>\n",
       "      <td>2602</td>\n",
       "      <td>2602</td>\n",
       "      <td>2602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kyle</th>\n",
       "      <td>7099</td>\n",
       "      <td>7099</td>\n",
       "      <td>7099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stan</th>\n",
       "      <td>7680</td>\n",
       "      <td>7680</td>\n",
       "      <td>7680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cartman</th>\n",
       "      <td>9774</td>\n",
       "      <td>9774</td>\n",
       "      <td>9774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Season  Episode  Line\n",
       "Character                          \n",
       "Mr. Mackey       633      633   633\n",
       "Sharon           862      862   862\n",
       "Kenny            881      881   881\n",
       "Chef             917      917   917\n",
       "Mr. Garrison    1002     1002  1002\n",
       "Randy           2467     2467  2467\n",
       "Butters         2602     2602  2602\n",
       "Kyle            7099     7099  7099\n",
       "Stan            7680     7680  7680\n",
       "Cartman         9774     9774  9774"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Character').count().sort_values(by='Season').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим из них только детей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28036</td>\n",
       "      <td>28036</td>\n",
       "      <td>28036</td>\n",
       "      <td>28036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>25502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>What?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2422</td>\n",
       "      <td>2233</td>\n",
       "      <td>9774</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season Episode Character     Line\n",
       "count   28036   28036     28036    28036\n",
       "unique     18      18         5    25502\n",
       "top         2       1   Cartman  What?\\n\n",
       "freq     2422    2233      9774      194"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_characters=['Cartman', 'Stan', 'Kyle', 'Butters', 'Kenny']\n",
    "df_main = df[df['Character'].isin(main_characters)]\n",
    "df_main.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем в качестве тестовой выборки последние четыре сезона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df_main[df_main['Season'].astype(int) >= 15]\n",
    "df_train = df_main[df_main['Season'].astype(int) < 15]\n",
    "\n",
    "y = np.array(df_train['Character'])\n",
    "y_test = np.array(df_test['Character'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим dummy-решение: равновероятный выбор между героями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.14      0.20      0.16       631\n",
      "    Cartman       0.39      0.19      0.26      1713\n",
      "      Kenny       0.03      0.23      0.05       124\n",
      "       Kyle       0.23      0.20      0.21      1092\n",
      "       Stan       0.20      0.19      0.20       894\n",
      "\n",
      "avg / total       0.27      0.20      0.22      4454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls_dummy = DummyClassifier('uniform').fit(np.zeros((len(y), 1)), y)\n",
    "y_pred = cls_dummy.predict(np.zeros((len(y_test), 1)))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве baseline решения возьмем NaiveBayes над ненормализованной (но токенизированной) таблицей bag-of-words.\n",
    "\n",
    "RandomForest размера 100 на этих данных уже работает достаточно долго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TokenTokenizer(object):\n",
    "    def __call__(self, string):\n",
    "        string = re.sub(r'(\\W)', r' \\1 ', string)\n",
    "        tokens = string.strip().split()\n",
    "        tokens = [token for token in tokens if token != '']\n",
    "        return tokens\n",
    "    \n",
    "class WordTokenizer(object):\n",
    "    def __call__(self, string):\n",
    "        tokens = re.split(r'\\W', string)\n",
    "        tokens = [token.lower() for token in tokens if token != '']\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 23582, vocab size: 11154\n",
      "Test set size: 4454\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=TokenTokenizer())\n",
    "X_bayes = vectorizer.fit_transform(df_train['Line'])\n",
    "print('Train set size: {}, vocab size: {}'.format(*X.shape))\n",
    "X_test_bayes = vectorizer.transform(df_test['Line'])\n",
    "print('Test set size: {}'.format(*X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.62      0.10      0.16       631\n",
      "    Cartman       0.51      0.64      0.57      1713\n",
      "      Kenny       0.87      0.67      0.76       124\n",
      "       Kyle       0.40      0.27      0.32      1092\n",
      "       Stan       0.30      0.46      0.36       894\n",
      "\n",
      "avg / total       0.47      0.44      0.41      4454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls_baseline = MultinomialNB()\n",
    "cls_baseline.fit(X_bayes, y)\n",
    "y_pred = cls_baseline.predict(X_test_bayes)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше попробуем 2 различных пути: с помощью feature_selection, ngrams и max/min df будем подбирать оптимальные параметры векторизации для линейной регрессии, и на основе составленных вручную признаков будем обучать random forest.\n",
    "\n",
    "Но сперва подумаем, как обработать данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на примеры реплик каждого из персонажей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63787</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Why?!  Why?! Why did you have to take them bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48732</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>How good?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57741</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Nah, I'll lose it for sure. You keep track of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62083</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Our platinum album ceremony. I spared no expen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35594</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Let's see, where have I been, where have I bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55404</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Cartman, wake up!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15709</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Well, my friends are worried that I'm showing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59040</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Liver medicine?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22264</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>Stan</td>\n",
       "      <td>\"On the morrow\"? What the fuck is wrong with K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47472</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Have you confessed all your sins yet?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33445</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>No. No, it'll be okay!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18923</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Wikileaks.  It says here his name is Wikileaks.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63886</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>You don't understand what my mom will do to me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Good thinking\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Cartman, let me out of this stupid net!!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59053</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>Butters</td>\n",
       "      <td>Oh. Well, you're good at adventurin', huh Stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>Butters</td>\n",
       "      <td>I'm going going, back back, to Cali Cali. Uhh,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13275</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Butters</td>\n",
       "      <td>Okay, thanks. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63861</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Butters</td>\n",
       "      <td>Nuh uh! Because my cloak is made of a... titan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65064</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Butters</td>\n",
       "      <td>No!  No, please!  Don't kill him! He's my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49344</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>Kenny</td>\n",
       "      <td>(Yeah!)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47078</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Kenny</td>\n",
       "      <td>(It's when you take your finger, and you stick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46814</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Kenny</td>\n",
       "      <td>(Ah, fuck you!) \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43806</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>Kenny</td>\n",
       "      <td>(Yes, I agree. I think Craig is a thousand tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38811</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Kenny</td>\n",
       "      <td>(Okay. I can do that.)  (Christ!) \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Season Episode Character  \\\n",
       "63787      8       1   Cartman   \n",
       "48732      4      14   Cartman   \n",
       "57741      6      12   Cartman   \n",
       "62083      7       9   Cartman   \n",
       "35594      2      11   Cartman   \n",
       "55404      6       4      Stan   \n",
       "15709     14      10      Stan   \n",
       "59040      6      16      Stan   \n",
       "22264     16      11      Stan   \n",
       "47472      4      10      Stan   \n",
       "33445      2       5      Kyle   \n",
       "18923     15      10      Kyle   \n",
       "63886      8       1      Kyle   \n",
       "4477      11       4      Kyle   \n",
       "877       10       4      Kyle   \n",
       "59053      6      16   Butters   \n",
       "2671      10      11   Butters   \n",
       "13275     13      14   Butters   \n",
       "63861      8       1   Butters   \n",
       "65064      8       5   Butters   \n",
       "49344      4      16     Kenny   \n",
       "47078      4       9     Kenny   \n",
       "46814      4       8     Kenny   \n",
       "43806      3      16     Kenny   \n",
       "38811      3       1     Kenny   \n",
       "\n",
       "                                                    Line  \n",
       "63787  Why?!  Why?! Why did you have to take them bot...  \n",
       "48732                                        How good?\\n  \n",
       "57741  Nah, I'll lose it for sure. You keep track of ...  \n",
       "62083  Our platinum album ceremony. I spared no expen...  \n",
       "35594  Let's see, where have I been, where have I bee...  \n",
       "55404                                Cartman, wake up!\\n  \n",
       "15709  Well, my friends are worried that I'm showing ...  \n",
       "59040                                  Liver medicine?\\n  \n",
       "22264  \"On the morrow\"? What the fuck is wrong with K...  \n",
       "47472            Have you confessed all your sins yet?\\n  \n",
       "33445                           No. No, it'll be okay!\\n  \n",
       "18923  Wikileaks.  It says here his name is Wikileaks.\\n  \n",
       "63886  You don't understand what my mom will do to me...  \n",
       "4477                                     Good thinking\\n  \n",
       "877           Cartman, let me out of this stupid net!!\\n  \n",
       "59053  Oh. Well, you're good at adventurin', huh Stan...  \n",
       "2671   I'm going going, back back, to Cali Cali. Uhh,...  \n",
       "13275                                   Okay, thanks. \\n  \n",
       "63861  Nuh uh! Because my cloak is made of a... titan...  \n",
       "65064  No!  No, please!  Don't kill him! He's my best...  \n",
       "49344                                          (Yeah!)\\n  \n",
       "47078  (It's when you take your finger, and you stick...  \n",
       "46814                                 (Ah, fuck you!) \\n  \n",
       "43806  (Yes, I agree. I think Craig is a thousand tim...  \n",
       "38811               (Okay. I can do that.)  (Christ!) \\n  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = []\n",
    "for character in main_characters:\n",
    "    df_sample.append(df_main[df_main['Character'] == character].sample(5))\n",
    "pandas.concat(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хм, все реплики Кенни выделены в скобки!\n",
    "(в мультсериале он говорит сквозь капюшон и его слова с трудом можно было разобрать).\n",
    "\n",
    "Используем эту особенность -- оставим токенизацию по знакам препинания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще из особенностей: короткие предложения с большой буквы: лучше привести к нижнему регистру и отдельно учесть длину/ количество слов в предложении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим частотные списки по каждому персонажу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stan: you, i, to, we, the, s, it, t, a, that, what, and, dude, is, this, re, on, of, have, do, in, just, no, he, yeah, don, all, can, are, my, oh, cartman, they, be, for, get, go, not, but, come, gonna, know, me, m, so, dad, with, us, your, now\n",
      "\n",
      "Cartman: you, i, the, to, s, a, it, and, that, t, we, of, is, my, what, me, in, this, m, on, have, all, oh, can, guys, for, re, just, no, kyle, your, do, are, don, get, be, with, so, right, now, not, out, here, go, gonna, he, know, they, like, but\n",
      "\n",
      "Kenny: i, you, yeah, s, that, the, what, it, to, a, oh, no, hey, and, me, guys, t, m, fuck, uh, my, we, woohoo, this, do, get, is, don, dude, okay, huh, on, of, they, not, fucking, all, have, are, god, so, now, too, ve, gonna, can, in, go, re, got\n",
      "\n",
      "Kyle: you, i, to, the, s, it, we, t, a, that, what, and, is, dude, of, cartman, have, re, this, can, do, on, in, don, he, yeah, just, no, all, my, are, for, get, be, not, they, me, but, your, with, m, oh, so, go, out, know, gonna, up, there, us\n",
      "\n",
      "Butters: i, you, the, a, s, to, it, and, uh, t, that, my, oh, well, me, m, we, what, of, in, is, do, but, can, on, hey, all, no, this, for, yeah, just, be, he, re, are, they, have, don, now, get, huh, go, gonna, know, like, not, there, with, see\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "popular_words = defaultdict(list)\n",
    "for character in main_characters:\n",
    "    char_lines = list(df_train[df_train['Character'] == character]['Line'])\n",
    "    char_lines = [WordTokenizer()(line) for line in char_lines]\n",
    "    char_words = [word for line in char_lines for word in line]\n",
    "    char_cnt = Counter(char_words)\n",
    "    popular_words[character] = [word for word, count in char_cnt.most_common(300)]\n",
    "    \n",
    "for character in popular_words:\n",
    "    print('{}: {}\\n'.format(character, ', '.join(popular_words[character][:50])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слишком много повторяющихся стоп-слов.\n",
    "Посмотрим на топ пересечений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 intersection:\n",
      "a, all, and, are, can, do, don, get, go, gonna, have, i, in, is, it, m, me, my, no, not, of, oh, on, re, s, t, that, the, they, this, to, we, what, you\n",
      "\n",
      "First 100 intersection:\n",
      "at, be, but, come, did, for, got, guys, he, here, hey, how, just, know, let, like, ll, look, now, okay, out, really, right, see, so, there, think, uh, ve, was, well, why, with, yeah, your\n",
      "\n",
      "First 200 intersection:\n",
      "about, again, as, aw, because, could, d, down, god, going, good, has, her, him, his, huh, if, jesus, kenny, little, mom, one, our, please, said, she, some, stupid, sure, take, then, time, too, up, wait, wanna, want, way, when, where, would\n"
     ]
    }
   ],
   "source": [
    "intersection_50 = set.intersection(*[set(words[:50]) for words in popular_words.values()])\n",
    "intersection_100 = set.intersection(*[set(words[:100]) for words in popular_words.values()])\n",
    "intersection_200 = set.intersection(*[set(words[:200]) for words in popular_words.values()])\n",
    "intersection_200.difference_update(intersection_100)\n",
    "intersection_100.difference_update(intersection_50)\n",
    "print('First 50 intersection:\\n' + ', '.join(sorted(intersection_50)))\n",
    "print('\\nFirst 100 intersection:\\n' + ', '.join(sorted(intersection_100)))\n",
    "print('\\nFirst 200 intersection:\\n' + ', '.join(sorted(intersection_200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:\n",
      "a, all, and, are, at, be, but, can, come, did, do, don, for, get, go, gonna, got, have, he, here, how, i, in, is, it, just, know, let, like, ll, look, m, me, my, no, not, now, of, oh, on, out, re, s, see, so, t, that, the, there, they, think, this, to, ve, was, we, well, what, why, with, you, your\n"
     ]
    }
   ],
   "source": [
    "stop_words = intersection_50.difference('oh')\n",
    "stop_words |= intersection_100.difference('guys hey okay really right uh yeah'.split())\n",
    "stop_words = list(sorted(stop_words))\n",
    "print('Stop words:\\n' + ', '.join(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на еще раз на частотные списки без стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stan: dude, yeah, cartman, dad, us, guys, kyle, our, up, right, about, hey, okay, kenny\n",
      "\n",
      "Cartman: guys, kyle, right, yeah, up, god, butters, if, kenny, about, okay, one\n",
      "\n",
      "Kenny: yeah, hey, guys, fuck, uh, woohoo, dude, okay, huh, fucking, god, too, o, right, cartman, fuckin, gotta, take, hoo\n",
      "\n",
      "Kyle: dude, cartman, yeah, up, us, stan, right, about, hey, if, him, kenny, really, people\n",
      "\n",
      "Butters: uh, hey, yeah, huh, eric, sure, ah, dad, fellas, o, if, right, up\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for character in popular_words:\n",
    "    print('{}: {}\\n'.format(character, ', '.join([w for w in popular_words[character][:70] if w not in stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "def normalize(text):\n",
    "    tokens = WordTokenizer()(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "df_main['Line_normalized'] = df_main['Line'].map(lambda text: normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 11054\n"
     ]
    }
   ],
   "source": [
    "df_vocab = df_main[df_main['Season'].astype(int) < 15]['Line_normalized']\n",
    "vocabulary = sorted(set(word for words in df_vocab for word in words))\n",
    "print('Vocabulary size: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с самых простых признаков: ранее мы поняли, что реплики Кенни легко можно отличить по наличию скобок.\n",
    "\n",
    "Добавим имена упоминаемых героев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_main['talk_Kenny'] = df_main['Line'].map(lambda text: int(text[0] + text.strip()[-1] == '()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for character in main_characters:\n",
    "    df_main['word_{}'.format(character)] = df_main['Line'].map(lambda text: int(character in text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.00      0.00      0.00       486\n",
      "    Cartman       0.36      0.98      0.53      2004\n",
      "      Kenny       0.99      0.97      0.98       208\n",
      "       Kyle       0.47      0.10      0.16      1488\n",
      "       Stan       0.71      0.01      0.01      1710\n",
      "\n",
      "avg / total       0.48      0.39      0.26      5896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_main[df_main['Season'].astype(int) < 15].ix[:, 5:])\n",
    "y = np.array(df_main[df_main['Season'].astype(int) < 15]['Character'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "cls_rf = RandomForestClassifier(n_estimators=100)\n",
    "cls_rf.fit(X_train, y_train)\n",
    "y_pred = cls_rf.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью всего 6 признаков мы получили неплохой precision, но f1-score хуже, чем у baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим синтаксические признаки и посмотрим на распределение по персонажам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "df_main['count_symbols'] = df_main['Line'].map(lambda text: len(text))\n",
    "df_main['count_words'] = df_main['Line'].map(lambda text: len(WordTokenizer()(text)))\n",
    "df_main['count_nonstopwords'] = df_main['Line_normalized'].map(lambda tokens: len(tokens))\n",
    "df_main['count_upper'] = df_main['Line'].map(lambda text: len([c for c in text if c in string.uppercase]))\n",
    "for sign in (\"!.,?'\"):\n",
    "    df_main['count_{}'.format(sign)] = df_main['Line'].map(lambda text: text.count(sign))\n",
    "df_main['count_digits'] = df_main['Line'].map(lambda text: len([c for c in text if c in '0123456789']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk_Kenny</th>\n",
       "      <th>word_Cartman</th>\n",
       "      <th>word_Stan</th>\n",
       "      <th>word_Kyle</th>\n",
       "      <th>word_Butters</th>\n",
       "      <th>word_Kenny</th>\n",
       "      <th>count_symbols</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_nonstopwords</th>\n",
       "      <th>count_upper</th>\n",
       "      <th>count_!</th>\n",
       "      <th>count_.</th>\n",
       "      <th>count_,</th>\n",
       "      <th>count_?</th>\n",
       "      <th>count_'</th>\n",
       "      <th>count_digits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Butters</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.021816</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>59.391172</td>\n",
       "      <td>11.995434</td>\n",
       "      <td>6.150685</td>\n",
       "      <td>3.170472</td>\n",
       "      <td>0.602232</td>\n",
       "      <td>1.312024</td>\n",
       "      <td>1.033486</td>\n",
       "      <td>0.358701</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.016743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cartman</th>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.005955</td>\n",
       "      <td>0.020717</td>\n",
       "      <td>0.067361</td>\n",
       "      <td>0.038829</td>\n",
       "      <td>0.031634</td>\n",
       "      <td>69.318199</td>\n",
       "      <td>13.706736</td>\n",
       "      <td>7.035479</td>\n",
       "      <td>3.406029</td>\n",
       "      <td>0.865153</td>\n",
       "      <td>1.304181</td>\n",
       "      <td>1.098003</td>\n",
       "      <td>0.357152</td>\n",
       "      <td>0.769508</td>\n",
       "      <td>0.051482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenny</th>\n",
       "      <td>0.977543</td>\n",
       "      <td>0.015852</td>\n",
       "      <td>0.005284</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>0.005284</td>\n",
       "      <td>26.446499</td>\n",
       "      <td>4.826948</td>\n",
       "      <td>2.484808</td>\n",
       "      <td>1.766182</td>\n",
       "      <td>0.632761</td>\n",
       "      <td>0.561427</td>\n",
       "      <td>0.397622</td>\n",
       "      <td>0.214003</td>\n",
       "      <td>0.303831</td>\n",
       "      <td>0.005284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kyle</th>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.076910</td>\n",
       "      <td>0.029299</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>46.884302</td>\n",
       "      <td>9.335109</td>\n",
       "      <td>4.634926</td>\n",
       "      <td>2.299484</td>\n",
       "      <td>0.560013</td>\n",
       "      <td>0.919594</td>\n",
       "      <td>0.648910</td>\n",
       "      <td>0.342101</td>\n",
       "      <td>0.562344</td>\n",
       "      <td>0.030797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stan</th>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.053050</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>0.027704</td>\n",
       "      <td>47.041851</td>\n",
       "      <td>9.410109</td>\n",
       "      <td>4.681698</td>\n",
       "      <td>2.178308</td>\n",
       "      <td>0.442529</td>\n",
       "      <td>0.938403</td>\n",
       "      <td>0.761273</td>\n",
       "      <td>0.333923</td>\n",
       "      <td>0.598585</td>\n",
       "      <td>0.027409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           talk_Kenny  word_Cartman  word_Stan  word_Kyle  word_Butters  \\\n",
       "Character                                                                 \n",
       "Butters      0.000000      0.008625   0.021816   0.015728      0.014713   \n",
       "Cartman      0.000248      0.005955   0.020717   0.067361      0.038829   \n",
       "Kenny        0.977543      0.015852   0.005284   0.009247      0.003963   \n",
       "Kyle         0.000333      0.076910   0.029299   0.003496      0.015315   \n",
       "Stan         0.000589      0.053050   0.003389   0.034483      0.016799   \n",
       "\n",
       "           word_Kenny  count_symbols  count_words  count_nonstopwords  \\\n",
       "Character                                                               \n",
       "Butters      0.006596      59.391172    11.995434            6.150685   \n",
       "Cartman      0.031634      69.318199    13.706736            7.035479   \n",
       "Kenny        0.005284      26.446499     4.826948            2.484808   \n",
       "Kyle         0.026969      46.884302     9.335109            4.634926   \n",
       "Stan         0.027704      47.041851     9.410109            4.681698   \n",
       "\n",
       "           count_upper   count_!   count_.   count_,   count_?   count_'  \\\n",
       "Character                                                                  \n",
       "Butters       3.170472  0.602232  1.312024  1.033486  0.358701  0.813800   \n",
       "Cartman       3.406029  0.865153  1.304181  1.098003  0.357152  0.769508   \n",
       "Kenny         1.766182  0.632761  0.561427  0.397622  0.214003  0.303831   \n",
       "Kyle          2.299484  0.560013  0.919594  0.648910  0.342101  0.562344   \n",
       "Stan          2.178308  0.442529  0.938403  0.761273  0.333923  0.598585   \n",
       "\n",
       "           count_digits  \n",
       "Character                \n",
       "Butters        0.016743  \n",
       "Cartman        0.051482  \n",
       "Kenny          0.005284  \n",
       "Kyle           0.030797  \n",
       "Stan           0.027409  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main[df_main['Season'].astype(int) < 15].groupby('Character').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.14      0.06      0.08       477\n",
      "    Cartman       0.45      0.56      0.50      2028\n",
      "      Kenny       0.98      0.97      0.97       186\n",
      "       Kyle       0.33      0.30      0.31      1488\n",
      "       Stan       0.37      0.35      0.36      1717\n",
      "\n",
      "avg / total       0.39      0.41      0.39      5896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_main[df_main['Season'].astype(int) < 15].ix[:, 5:])\n",
    "y = np.array(df_main[df_main['Season'].astype(int) < 15]['Character'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "cls_rf = RandomForestClassifier(n_estimators=100)\n",
    "cls_rf.fit(X_train, y_train)\n",
    "y_pred = cls_rf.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Незначительное улучшение есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим частотные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popular_words_filtered = {char : [w for w in words if w not in stop_words] for char, words in popular_words.items()}\n",
    "\n",
    "for words in popular_words_filtered:\n",
    "    for word in words[:200]:\n",
    "        df_main['word_{}'.format(word)] = df_main['Line_normalized'].map(lambda tokens: int(word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.14      0.06      0.08       505\n",
      "    Cartman       0.44      0.56      0.49      1980\n",
      "      Kenny       0.99      0.97      0.98       181\n",
      "       Kyle       0.35      0.30      0.32      1488\n",
      "       Stan       0.37      0.37      0.37      1742\n",
      "\n",
      "avg / total       0.39      0.41      0.39      5896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_main[df_main['Season'].astype(int) < 15].ix[:, 5:])\n",
    "y = np.array(df_main[df_main['Season'].astype(int) < 15]['Character'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "cls_rf = RandomForestClassifier(n_estimators=100)\n",
    "cls_rf.fit(X_train, y_train)\n",
    "y_pred = cls_rf.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увы, из hand-crafted признаков не удалось получить ничего стоящего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df_main[df_main['Season'].astype(int) >= 15]\n",
    "df_train = df_main[df_main['Season'].astype(int) < 15]\n",
    "\n",
    "y = np.array(df_train['Character'])\n",
    "y_test = np.array(df_test['Character'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить логистическую регрессию при базовой обработке текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.58      0.33      0.42       464\n",
      "    Cartman       0.57      0.63      0.60      2025\n",
      "      Kenny       0.97      0.98      0.98       187\n",
      "       Kyle       0.42      0.34      0.38      1531\n",
      "       Stan       0.44      0.52      0.47      1689\n",
      "\n",
      "avg / total       0.51      0.51      0.50      5896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=TokenTokenizer())\n",
    "X = vectorizer.fit_transform(df_train['Line'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "X_test_log = vectorizer.transform(df_test['Line'])\n",
    "cls_linear = LogisticRegression(random_state=42)\n",
    "cls_linear.fit(X_train, y_train)\n",
    "y_pred = cls_linear.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично, вместо LogisticRegression можно использовать SGDClassifier.\n",
    "\n",
    "Он немного быстрее на большом объеме данных и позволяет выбрать функцию ошибки.\n",
    "\n",
    "Например, 'log' соотвествует логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.62      0.34      0.44       500\n",
      "    Cartman       0.54      0.65      0.59      1996\n",
      "      Kenny       0.97      0.99      0.98       185\n",
      "       Kyle       0.44      0.36      0.40      1476\n",
      "       Stan       0.46      0.49      0.47      1739\n",
      "\n",
      "avg / total       0.51      0.51      0.51      5896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=TokenTokenizer())\n",
    "X = vectorizer.fit_transform(df_train['Line'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "cls_sgd = SGDClassifier(\n",
    "    loss='log',\n",
    "    n_iter=50,                \n",
    "    random_state=42,\n",
    "    shuffle=True)\n",
    "cls_sgd.fit(X_train, y_train)\n",
    "y_pred = cls_sgd.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот лучший результат, который удалось получить на валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 23582, vocab size: 50000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.74      0.21      0.32       498\n",
      "    Cartman       0.58      0.78      0.67      2025\n",
      "      Kenny       0.97      0.99      0.98       187\n",
      "       Kyle       0.56      0.41      0.48      1482\n",
      "       Stan       0.54      0.57      0.56      1704\n",
      "\n",
      "avg / total       0.59      0.58      0.57      5896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=TokenTokenizer(),\n",
    "    ngram_range=(1, 3),\n",
    "    max_df=0.25)\n",
    "X = vectorizer.fit_transform(df_train['Line'])\n",
    "feature_selector = SelectKBest(f_classif, k=50000)\n",
    "X = feature_selector.fit_transform(X, y)\n",
    "print('Train set size: {}, vocab size: {}'.format(*X.shape))\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "cls_sgd = SGDClassifier(\n",
    "    loss='modified_huber',\n",
    "    n_iter=50,                \n",
    "    random_state=42,\n",
    "    shuffle=True)\n",
    "cls_sgd.fit(X_train, y_train)\n",
    "y_pred = cls_sgd.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 23582, vocab size: 50000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.77      0.29      0.42       480\n",
      "    Cartman       0.59      0.76      0.67      2044\n",
      "      Kenny       0.98      0.98      0.98       193\n",
      "       Kyle       0.53      0.44      0.48      1472\n",
      "       Stan       0.55      0.54      0.55      1707\n",
      "\n",
      "avg / total       0.59      0.59      0.58      5896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=TokenTokenizer(),\n",
    "    ngram_range=(1, 3),\n",
    "    max_df=0.25)\n",
    "X_sgd = vectorizer.fit_transform(df_train['Line'])\n",
    "X_test_sgd = vectorizer.transform(df_test['Line'])\n",
    "feature_selector = SelectKBest(f_classif, k=50000)\n",
    "X_sgd = feature_selector.fit_transform(X_sgd, y)\n",
    "X_test_sgd = feature_selector.transform(X_test_sgd)\n",
    "print('Train set size: {}, vocab size: {}'.format(*X_sgd.shape))\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_sgd, y)\n",
    "cls_sgd = SGDClassifier(\n",
    "    loss='modified_huber',\n",
    "    n_iter=50,                \n",
    "    random_state=42,\n",
    "    shuffle=True)\n",
    "cls_sgd.fit(X_train, y_train)\n",
    "y_pred = cls_sgd.predict(X_valid)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У всех наших моделей была разная предобработка, поэтому придется использовать три разные таблицы X_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_rf = np.array(df_main[df_main['Season'].astype(int) >= 15].ix[:, 5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_bayes = cls_baseline.predict(X_test_bayes)\n",
    "y_proba_bayes = cls_baseline.predict_proba(X_test_bayes)\n",
    "\n",
    "y_pred_rf = cls_rf.predict(X_test_rf)\n",
    "y_proba_rf = cls_rf.predict_proba(X_test_rf)\n",
    "\n",
    "y_pred_sgd = cls_sgd.predict(X_test_sgd)\n",
    "y_proba_sgd = cls_sgd.predict_proba(X_test_sgd)\n",
    "\n",
    "y_pred_log = cls_linear.predict(X_test_log)\n",
    "y_proba_log = cls_linear.predict_proba(X_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.62      0.10      0.16       631\n",
      "    Cartman       0.51      0.64      0.57      1713\n",
      "      Kenny       0.87      0.67      0.76       124\n",
      "       Kyle       0.40      0.27      0.32      1092\n",
      "       Stan       0.30      0.46      0.36       894\n",
      "\n",
      "avg / total       0.47      0.44      0.41      4454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Bayes classification report')\n",
    "print(classification_report(y_test, y_pred_bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.21      0.04      0.07       631\n",
      "    Cartman       0.48      0.59      0.53      1713\n",
      "      Kenny       0.86      0.77      0.81       124\n",
      "       Kyle       0.35      0.30      0.32      1092\n",
      "       Stan       0.26      0.34      0.30       894\n",
      "\n",
      "avg / total       0.37      0.39      0.37      4454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Random forest classification report')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.65      0.13      0.22       631\n",
      "    Cartman       0.52      0.67      0.58      1713\n",
      "      Kenny       0.88      0.81      0.85       124\n",
      "       Kyle       0.40      0.31      0.35      1092\n",
      "       Stan       0.30      0.38      0.34       894\n",
      "\n",
      "avg / total       0.47      0.45      0.43      4454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SGD linear model classification report')\n",
    "print(classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regession classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Butters       0.61      0.17      0.27       631\n",
      "    Cartman       0.55      0.61      0.58      1713\n",
      "      Kenny       0.88      0.81      0.85       124\n",
      "       Kyle       0.41      0.35      0.38      1092\n",
      "       Stan       0.30      0.45      0.36       894\n",
      "\n",
      "avg / total       0.48      0.46      0.45      4454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Logistic regession classification report')\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, SGDClassifier, показавший себя лучше всего на валидации, к сожалению, переобучился, и лучшим результатом на тесте стала логистическая регрессиия."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
